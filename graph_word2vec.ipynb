{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasinEnigma/Scripts/blob/master/graph_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/amnghd/Persian_poems_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpAbmkcksa1G",
        "outputId": "7848e0ca-8b71-4e61-91b3-486f81175efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Persian_poems_corpus'...\n",
            "remote: Enumerating objects: 159, done.\u001b[K\n",
            "remote: Total 159 (delta 0), reused 0 (delta 0), pack-reused 159\u001b[K\n",
            "Receiving objects: 100% (159/159), 45.21 MiB | 18.24 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (148/148), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv Persian_poems_corpus/original/ ./"
      ],
      "metadata": {
        "id": "BrOPoitGsmcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir normalized 'stop words removed'"
      ],
      "metadata": {
        "id": "aHoRIvsfswiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install node2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nTUbU0gsbyy",
        "outputId": "e794f78a-2537-4e17-e33a-a09c972aa48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting node2vec\n",
            "  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.3.1)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.2.0)\n",
            "Collecting networkx<3.0,>=2.5 (from node2vec)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.22.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.65.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.1.2->node2vec) (6.3.0)\n",
            "Installing collected packages: networkx, node2vec\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "Successfully installed networkx-2.8.8 node2vec-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pecanpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrqdobD9aFIH",
        "outputId": "ad257ea4-6030-4042-8325-15c100ccaa21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pecanpy\n",
            "  Downloading pecanpy-2.0.8-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: gensim>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from pecanpy) (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from pecanpy) (1.22.4)\n",
            "Requirement already satisfied: numba>=0.46.0 in /usr/local/lib/python3.10/dist-packages (from pecanpy) (0.56.4)\n",
            "Collecting numba-progress>=0.0.2 (from pecanpy)\n",
            "  Downloading numba_progress-0.0.4-py3-none-any.whl (8.1 kB)\n",
            "Collecting nptyping>=2.0.0 (from pecanpy)\n",
            "  Downloading nptyping-2.5.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.1.0->pecanpy) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.1.0->pecanpy) (6.3.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.46.0->pecanpy) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.46.0->pecanpy) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from numba-progress>=0.0.2->pecanpy) (4.65.0)\n",
            "Installing collected packages: nptyping, numba-progress, pecanpy\n",
            "Successfully installed nptyping-2.5.0 numba-progress-0.0.4 pecanpy-2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX2y3X2xB4K7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMhLkMI5B4K_"
      },
      "source": [
        "# below is a context graph, that represents words that are adjencent to each other with count as weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M2uzNl0B4LB"
      },
      "outputs": [],
      "source": [
        "from glob import glob # using glob to input all the poem files\n",
        "from pers_alphab import pers_alphab # used to normalize the poem alphabet\n",
        "import re # for quick split function\n",
        "from time import time # to time the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd7yj7hhB4LD"
      },
      "outputs": [],
      "source": [
        "def stop_word_importer(file_name):# importing persian stopwords\n",
        "    with open(file_name, 'r', encoding=\"utf8\") as myfile:\n",
        "        stop_words = myfile.read().replace('\\n', ' ').replace(\"\\u200c\",\"\").replace(\"\\ufeff\",\"\").replace(\".\",\" \").split(' ')# a list of stop words\n",
        "    return stop_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVVzDVKqB4LE"
      },
      "outputs": [],
      "source": [
        "original_path = 'original/*.txt'  # where the original (scraped) files exist\n",
        "poet_files = glob(original_path)  # captures list of all the files in the folder\n",
        "stop_words = stop_word_importer('stop_words.txt') # importing stop words to a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUgli6u9B4LG"
      },
      "outputs": [],
      "source": [
        "def document_traverser(original_path, normalized_path, stop_remv_path):\n",
        "    # one read two writes are initiated\n",
        "    with open(original_path, \"r\", encoding=\"utf8\") as f,\\\n",
        "    open(normalized_path, \"w\",  encoding=\"utf8\") as n,\\\n",
        "    open(stop_remv_path, \"w\",  encoding=\"utf8\") as s:\n",
        "        for line in f:\n",
        "            normalized = pers_alphab(line) # normelizing the content\n",
        "            word_list = re.split('[\\t\\s:]+', normalized) # tokenizing the content\n",
        "            cleaned_words = [x for x in word_list if x not in stop_words] # no stop word is in it\n",
        "            n.write(normalized) # writing out the normalized documents\n",
        "            s.write(\" \".join(cleaned_words) + '\\n')  # writing out the stop word removed documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0eoDblyB4LG"
      },
      "outputs": [],
      "source": [
        "for poet in poet_files: # going over the list of all poems\n",
        "    poet_name = poet[9:-4] # taking poems name out for output files\n",
        "    document_traverser(poet, \"normalized/\" + poet_name + \"_norm.txt\",\n",
        "                       \"stop words removed/\" + poet_name + \"_stp_rmv.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r normalized/ poems/"
      ],
      "metadata": {
        "id": "ekxshZWkCvo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0DgTaseB4LI"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "read_files = glob.glob(\"poems/*.txt\") # reading all the files into one list\n",
        "\n",
        "with open(\"poems/whole_farsi_corpus.txt\", \"w\", encoding=\"utf8\") as outfile:\n",
        "    for f in read_files:\n",
        "        with open(f, \"r\", encoding=\"utf8\") as infile:\n",
        "            outfile.write(infile.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cXgBgJJB4LJ",
        "outputId": "5e0aee7b-fb73-4e19-9d82-6b75e970f80d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['poems/obeyd_norm.txt',\n",
              " 'poems/saeb_norm.txt',\n",
              " 'poems/khajoo_norm.txt',\n",
              " 'poems/parvin_norm.txt',\n",
              " 'poems/saadi_norm.txt',\n",
              " 'poems/attar_norm.txt',\n",
              " 'poems/khosro_norm.txt',\n",
              " 'poems/ouhadi_norm.txt',\n",
              " 'poems/bidel_norm.txt',\n",
              " 'poems/shahnematollah_norm.txt',\n",
              " 'poems/asad_norm.txt',\n",
              " 'poems/sanaee_norm.txt',\n",
              " 'poems/hatef_norm.txt',\n",
              " 'poems/ghaani_norm.txt',\n",
              " 'poems/moulavi_norm.txt',\n",
              " 'poems/gilani_norm.txt',\n",
              " 'poems/babaafzal_norm.txt',\n",
              " 'poems/rahi_norm.txt',\n",
              " 'poems/abusaeed_norm.txt',\n",
              " 'poems/naserkhosro_norm.txt',\n",
              " 'poems/orfi_norm.txt',\n",
              " 'poems/khayyam_norm.txt',\n",
              " 'poems/zahir_norm.txt',\n",
              " 'poems/razi_norm.txt',\n",
              " 'poems/shabestari_norm.txt',\n",
              " 'poems/asadi_norm.txt',\n",
              " 'poems/bahaee_norm.txt',\n",
              " 'poems/farrokhi_norm.txt',\n",
              " 'poems/hafez_norm.txt',\n",
              " 'poems/helali_norm.txt',\n",
              " 'poems/salman_norm.txt',\n",
              " 'poems/feyz_norm.txt',\n",
              " 'poems/seyf_norm.txt',\n",
              " 'poems/nezari_norm.txt',\n",
              " 'poems/vahshi_norm.txt',\n",
              " 'poems/shahriar_norm.txt',\n",
              " 'poems/kamal_norm.txt',\n",
              " 'poems/iqbal_norm.txt',\n",
              " 'poems/manoochehri_norm.txt',\n",
              " 'poems/khaghani_norm.txt',\n",
              " 'poems/onsori_norm.txt',\n",
              " 'poems/ferdousi_norm.txt',\n",
              " 'poems/eraghi_norm.txt',\n",
              " 'poems/amir_norm.txt',\n",
              " 'poems/bahar_norm.txt',\n",
              " 'poems/anvari_norm.txt',\n",
              " 'poems/roodaki_norm.txt',\n",
              " 'poems/jami_norm.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "read_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMaHI_bkB4LM",
        "outputId": "04f8b9e9-5d81-4cb7-8fb9-c5a0f97f508b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1216286 number of mesras in this document.\n",
            "Mesra length range from 1 words to 18 words.\n",
            "There are 148558 unique words in this document.\n",
            "There are 36 unique characters in this document.\n",
            "In total there are 8102119 words in this document.\n"
          ]
        }
      ],
      "source": [
        "import re  # to perform a couple of simple regexes\n",
        "from gensim.models import Word2Vec  # to perform word2vec modelling\n",
        "from time import time  # to monitor the run time\n",
        "\n",
        "tic = time()  # starting time\n",
        "poems_path = r\"poems/\"\n",
        "poet_name = \"whole_farsi_corpus.txt\"\n",
        "pattern = re.compile(r\"[\\n\\t]+\")  # splitting on tabs or lines (multiples)\n",
        "\n",
        "with open(poems_path + poet_name, 'r', encoding=\"utf8\") as file:\n",
        "    raw_string = file.read()  # reading the poet document into a string\n",
        "    raw_string = pattern.sub(raw_string, \"\\n\")  # replacing all\\n\\t with\\n\n",
        "    raw_string = raw_string.replace(\"[\", \"\").replace(\"]\", \"\")  # removes [ , ]\n",
        "    unique_chars = set(raw_string) - set(\"\\n\\s\\t \")  # generate unique chars\n",
        "    mesras = raw_string.split(\"\\n\")  # getting a list of mesras(clauses)\n",
        "    words = re.split(r\"[\\n\\s\\t ]+\", raw_string)  # getting a list of words\n",
        "    unique_words = set(words)  # unique words\n",
        "\n",
        "\n",
        "list_of_sentences = []  # developing a list of sentences to be used for Gsnim\n",
        "sentence_max_len = 0  # maximum length of the sentences initiaton\n",
        "sentence_min_len = 100  # minimum length of the sentences initiaton\n",
        "for mesra in mesras:\n",
        "    if mesra.strip():  # not an empty line is faced\n",
        "        mesra_list = [word for word in mesra.strip().split(\" \") if word]\n",
        "        list_of_sentences.append(mesra_list)\n",
        "        if len(mesra_list) > sentence_max_len:\n",
        "            sentence_max_len = len(mesra_list)  # for troubleshooting\n",
        "        if len(mesra_list) < sentence_min_len:\n",
        "            sentence_min_len = len(mesra_list)  # for troubleshooting\n",
        "\n",
        "cache = {}  # to hold informations to be transferred\n",
        "cache[\"num_of_uwords\"] = len(unique_words)  # caching number of unique words\n",
        "cache[\"num_of_chars\"] = len(unique_chars)  # cachine number of characarcters\n",
        "cache[\"num_of_words\"] = len(words)  # total number of words\n",
        "cache[\"unique_words\"] = unique_words  # unique words to be used\n",
        "cache[\"sentence_min_max\"] = (sentence_min_len, sentence_max_len)  # range\n",
        "cache[\"num_mesra\"] = len(list_of_sentences)  # number of mesras\n",
        "\n",
        "print(\"There are {} number of mesras in this document.\".format(cache[\"num_mesra\"]))\n",
        "print(\"Mesra length range from {} words to {} words.\".format(sentence_min_len, sentence_max_len))\n",
        "print(\"There are {} unique words in this document.\".format(cache[\"num_of_uwords\"]))\n",
        "print(\"There are {} unique characters in this document.\".format(cache[\"num_of_chars\"]))\n",
        "print(\"In total there are {} words in this document.\".format(cache[\"num_of_words\"]))\n",
        "\n",
        "# following hyper parameters are checked to finalize propoer hyperparameters\n",
        "# size in [10, 100, 1000]\n",
        "# window in [5, 40, 320]\n",
        "# min_count in [1, 10, 100]\n",
        "# developing the word2vec funtion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e489Az9NB4LN"
      },
      "outputs": [],
      "source": [
        "data = list_of_sentences.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOWXH3UVB4LO"
      },
      "outputs": [],
      "source": [
        "G =  nx.DiGraph()\n",
        "for s in data:\n",
        "    G.add_nodes_from(s)\n",
        "    for i,w in enumerate(s[1:]):\n",
        "        if (s[i],w) not in G.edges:\n",
        "            G.add_edge(s[i],w,weight=1)\n",
        "        else:\n",
        "            G[s[i]][w][\"weight\"]+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAgD83AyB4LP",
        "outputId": "e8f9030b-299e-4cde-9975-82ea05f986c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "148558"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(list(G.nodes()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6ZTkGyNB4LQ"
      },
      "outputs": [],
      "source": [
        "vocabulary = [\"NA\"] + list(G.nodes)\n",
        "vocabulary_lookup = {token: idx for idx, token in enumerate(vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxPt2XlVB4LS",
        "outputId": "51385ff0-3998-4dca-dcdc-f0753267e70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of graph nodes: 148558\n",
            "Total number of graph edges: 2171499\n"
          ]
        }
      ],
      "source": [
        "print(\"Total number of graph nodes:\", G.number_of_nodes())\n",
        "print(\"Total number of graph edges:\", G.number_of_edges())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Density is: ', (G.number_of_edges()/(G.number_of_nodes()**2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_P3RPBvDWks",
        "outputId": "d715c9aa-eac2-4bf5-d1be-3fb25925905a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Density is:  9.839375746861411e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2xOe8RNB4LT",
        "outputId": "681c3c35-f082-4ebf-8ea5-c12357631704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average node degree: 29.23\n"
          ]
        }
      ],
      "source": [
        "degrees = []\n",
        "for node in G.nodes:\n",
        "    degrees.append(G.degree[node])\n",
        "\n",
        "print(\"Average node degree:\", round(sum(degrees) / len(degrees), 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtT4QA-9B4LV"
      },
      "outputs": [],
      "source": [
        "nx.write_edgelist(G, \"graph.edgelist\", data=[\"weight\"], encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldVXaEwSB4LV"
      },
      "outputs": [],
      "source": [
        "from pecanpy import pecanpy# load graph object using SparseOTF mode\n",
        "g = pecanpy.SparseOTF(p=0.8, q=0.1, workers=4, verbose=False)\n",
        "g.read_edg(\"graph.edgelist\", weighted=True, directed=True, delimiter=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UByrtweSB4LW"
      },
      "outputs": [],
      "source": [
        "# generate random walks\n",
        "walks = g.simulate_walks(num_walks=30, walk_length=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = 100\n",
        "window = 5\n",
        "min_count =100\n",
        "w2v_model = Word2Vec(\n",
        "                walks,\n",
        "                sg=1,\n",
        "                vector_size=size,\n",
        "                window=window,\n",
        "                min_count=min_count,\n",
        "                workers=4)\n",
        "w2v_model.train(walks, total_examples=len(walks), epochs=10)"
      ],
      "metadata": {
        "id": "3seQxaRgfScn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b042de-ae11-4ce5-d3f8-422614dd53c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1267160180, 1416409090)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K42YdN-gB4LW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "b8dd788f-92d3-4dc6-d57d-66c782279d98"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-2ccba77d93fb>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    w2v_model.wv.save_word2vec_format(pos1 = u'زلیخا'  # reason\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "# saving the model vector into a txt file\n",
        "w2v_model.wv.save_word2vec_format(pos1 = u'زلیخا'  # reason\n",
        "pos2 = u'مجنون'\n",
        "neg = u'یوسف'\n",
        "result = w2v_model.wv.most_similar(positive=[pos1, pos2], negative=[neg])\n",
        "print(neg + u\" به \" + pos1 + \" شبیه \" + pos2 + \" است:\\n{}\".format(*result[0]))\n",
        ", binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)"
      ],
      "metadata": {
        "id": "NBUd5KgkNKkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5e5b-rnB4LX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513ac3c4-734a-4e0a-fcd8-8c9b36b469ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few most similar words to \"خدا\" are:\n",
            "پندری:0.794999897480011\n",
            "جوزایه:0.6950055956840515\n",
            "پیدایه:0.6883607506752014\n"
          ]
        }
      ],
      "source": [
        "w1 = u'خدا'\n",
        "print(\"Few most similar words to \\\"\" + w1 + \"\\\" are:\\n\" + \"\\n\".join(str(word) +\":\" + str(ratio) for word, ratio in w2v_model.wv.most_similar(positive=w1, topn=3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi-I_-tqB4LX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18bcdab-c3b6-4462-bce5-5a9ea59b7283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few most similar words to \"عشق\" are:\n",
            "داغداری:0.7580649852752686\n",
            "عاشقی:0.6385080814361572\n",
            "عشقت:0.6222206354141235\n"
          ]
        }
      ],
      "source": [
        "w1 = u'عشق'\n",
        "print(\"Few most similar words to \\\"\" + w1 + \"\\\" are:\\n\" + \"\\n\".join(str(word) +\":\" + str(ratio) for word, ratio in w2v_model.wv.most_similar(positive=w1, topn=3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff3rymeZB4LY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76ed90c-ae1b-4a72-9e59-b864ac0c778a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few most similar words to \"حکایت\" are:\n",
            "قصه:0.734473466873169\n",
            "جویانم:0.6380172967910767\n",
            "حدیث:0.6314422488212585\n"
          ]
        }
      ],
      "source": [
        "w1 = u'حکایت'\n",
        "print(\"Few most similar words to \\\"\" + w1 + \"\\\" are:\\n\" + \"\\n\".join(str(word) +\":\" + str(ratio) for word, ratio in w2v_model.wv.most_similar(positive=w1, topn=3)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = u'شهر'\n",
        "print(\"Few most similar words to \\\"\" + w1 + \"\\\" are:\\n\" + \"\\n\".join(str(word) +\":\" + str(ratio) for word, ratio in w2v_model.wv.most_similar(positive=w1, topn=3)))"
      ],
      "metadata": {
        "id": "ZqLSG2OhtnYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b399dcc-26cf-4ab3-f8cf-5698aeb85898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few most similar words to \"شهر\" are:\n",
            "خطه:0.699218213558197\n",
            "کجاران:0.674374520778656\n",
            "آنجایند:0.642934262752533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = u'رستم'\n",
        "print(\"Few most similar words to \\\"\" + w1 + \"\\\" are:\\n\" + \"\\n\".join(str(word) +\":\" + str(ratio) for word, ratio in w2v_model.wv.most_similar(positive=w1, topn=5)))"
      ],
      "metadata": {
        "id": "C_Cukxw8t0mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb98cc7-ba25-4c45-b678-2b388fd95384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few most similar words to \"رستم\" are:\n",
            "زابلیست:0.8165891766548157\n",
            "زال:0.7275353670120239\n",
            "وطابران:0.7157247066497803\n",
            "دستان:0.7041464447975159\n",
            "نریمان:0.7017806768417358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrBG9S0SB4LZ"
      },
      "outputs": [],
      "source": [
        "pos1 = u'زن'  # reason\n",
        "pos2 = u'آدم'\n",
        "neg = u'مرد'\n",
        "result = w2v_model.wv.most_similar(positive=[pos1, pos2], negative=[neg])\n",
        "print(neg + u\" به \" + pos1 + \" شبیه \" + pos2 + \" است:\\n{}\".format(*result[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhcynwbdB4La"
      },
      "outputs": [],
      "source": [
        "# let's perform an anlogy test between ismaeil, ibrahim, hossein, ali\n",
        "pos1 = u'چشم'  # reason\n",
        "pos2 = u'شنیده'\n",
        "neg = u'گوش'\n",
        "result = w2v_model.wv.most_similar(positive=[pos1, pos2], negative=[neg])\n",
        "print(neg + u\" به \" + pos1 + \" شبیه \" + pos2 + \" است:\\n{}\".format(*result[0]))\n",
        "\n",
        "\n",
        "# this example is very important\n",
        "\n",
        "pos1 = u'رستم'  # reason\n",
        "pos2 = u'پدر'\n",
        "neg = u'زال'\n",
        "result = w2v_model.wv.most_similar(positive=[pos1, pos2], negative=[neg])\n",
        "print(neg + u\" به \" + pos1 + \" شبیه \" + pos2 + \" است:\\n{}\".format(*result[0]))\n",
        "\n",
        "\n",
        "pos1 = u'زلیخا'  # reason\n",
        "pos2 = u'مجنون'\n",
        "neg = u'یوسف'\n",
        "result = w2v_model.wv.most_similar(positive=[pos1, pos2], negative=[neg])\n",
        "print(neg + u\" به \" + pos1 + \" شبیه \" + pos2 + \" است:\\n{}\".format(*result[0]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pos1 = u'راست'  # reason\n",
        "pos2 = u'بالا'\n",
        "neg = u'چپ'\n",
        "result = w2v_model.wv.most_similar(positive=[pos1, pos2], negative=[neg])\n",
        "print(neg + u\" به \" + pos1 + \" شبیه \" + pos2 + \" است:\\n{}\".format(*result[0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "prfM29gOtaQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M5q8hbfdtiZd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}